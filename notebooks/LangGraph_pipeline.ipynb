{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "from utils.preprocess_transcription import remove_ads, identify_host, insert_marker_before_host\n",
    "from utils.extract_comp_guest import re_extract_comp_guest\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.chat_models import ChatAnthropic\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "# from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser, PydanticOutputParser\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient(os.getenv('mdb_uri'))\n",
    "DB_NAME = \"hibt_prod_db\"\n",
    "COLLECTION_NAME = \"hibt_prod_collection\"\n",
    "ATLAS_VECTOR_SEARCH_INDEX_NAME = \"vector_index\"\n",
    "MONGODB_COLLECTION = client[DB_NAME][COLLECTION_NAME]\n",
    "MONGODB_ANSWER_COLLECTION = client[DB_NAME][\"hibt_answer_collection\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_grading_prompt = \"\"\" You are evaluating a podcast transcript segment to determine if it contains a discussion relevant to a specific question. Your task is to assess whether the segment includes both a question from the host and an answer from a guest that relate closely to the provided relevant question.\n",
    "\n",
    "Transcript segment:\n",
    "{transcript}\n",
    "\n",
    "Relevant question to compare:\n",
    "{question}\n",
    "\n",
    "Your job is to decide if the segment discusses a question and answer similar to the relevant question provided. You need to give a binary score indicating the relevance.\n",
    "\n",
    "- If the segment contains a question and answer that closely match the relevant question, grade it as 'yes'.\n",
    "- If the segment does not contain a question and answer that are relevant, grade it as 'no'.\n",
    "\n",
    "Provide your assessment in the following JSON format, with only the 'score' key and your binary decision as the value. No preamble or explanation needed.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 'yes'}\n",
      "\n",
      "0 (7m 25s):\n",
      "When you were, when you were little, did you, would you dream about what you would be when you grew up or did you already have big ambitions as a kid or, or were they kind of like, yeah. You know, I'll, I'll see what happens\n",
      "\n",
      "1 (7m 40s):\n",
      "Now. See, print media was really the thing that taught me so much and books, print, period. My mom bought encyclopedias and then she paid them off over time. She also had periodicals that came to our house. There was a magazine called sippy. There was Ebony was popular. Jet magazine was popular back then. And so I knew a lot about the world and I knew a lot about what black people were doing. And so I had role models who, a fulfilled thoughts about what I might do when I grew up. But as a very young child, I remember admiring my mom so much, which I continue to do to this day.\n",
      "\n",
      "1 (8m 21s):\n",
      "She, even then I could sense, even though I couldn't put label too, it was quite an efficient person. And she ran our home like a business. And what is your dad do? My dad was a foreman. He was a foreman in a diaper factory, you know, were a textile communities in North Carolina. And dad was very integrated in our lives. He made sure that we had our Thursday family meetings. Thursday was payday and we all had to gather around and we discussed what we done, what we would be doing in business. We say we discussed the gaps and we brought forward at the solutions and my family, dad just see it.\n",
      "\n",
      "1 (9m 3s):\n",
      "Let's see where we are and where we're going, kids. That was always his comment to us.\n",
      "\n",
      "3 (9m 8s):\n",
      "Janice when you were, when you were a girl, when you were a kid, did you see entrepreneurs around you, either in your community or, or in your family?\n",
      "\n",
      "1 (9m 16s):\n",
      "Expectedly? I saw them growing up. I did not know that's what I was looking at. I remember grandma Dora walking around Tamora in high heel shoes, coming over to our house and they ran a barbecue shop and they had a home that had a beautiful dining room in it, but their dining room was to serve their customers. Well, they ran it out of their home. Yeah. And white people would come over to eat beer, which was a big deal that they were coming over to have dinner at the art lunch there. But I didn't look at that as entrepreneurship. Right, man. That's just what grandma did. You know, I also saw her run.\n",
      "\n",
      "1 (9m 57s):\n",
      "She fed people to go bags where they would come and buy their lunches or dinners at the back. And she would charge based on who was working and who was out of work. And she had that sense of justice in her mind about how to run her business. And I think it paid off because she did quite well.\n",
      "\n",
      "3 (10m 16s):\n",
      "Hmm. So such as I read that when you were in high school, your mom and dad decided to send you to a, an all white high school, you we're going to be the first African-American student at that school. What do you remember about, about your first day going in there?\n",
      "\n",
      "1 (10m 32s):\n",
      "Well, mama and daddy didn't send me. We decided as a family that that would happen. Our community wanted to have our best and brightest go to the white school to lessen any fear around whether or not the school with a dilapidated in any way by our attending it. And I was one of the first and I remember my first day going into my English class and I thought, Oh, English is always been one of my better subjects. And I'm going to fail because I couldn't understand my English teacher. She had a Twain that sounded so different to me, from my side of Tarboro, mind you, we're talking about a distance that I could walk to school from my home.\n",
      "\n",
      "1 (11m 20s):\n",
      "Wow. But she sounded so different. The communities were truly divided in that way. Yeah. And we had a teacher who stood up on the desk that date, the first day I went into his history class and explained so eloquently. If you can even see the paradox that, that How blacks were so suited to slavery. Wow. I remember chewing so hard saying, God, please don't let me cry. If you just let me get out of here without I'll never come back. That's how intimidated, how fearful In how foreign I felt and a U S history,\n",
      "\n",
      "3 (12m 1s):\n",
      "The class. Did you want to stop going to that school? I mean, I could imagine being 16 year old, 15, 16 year old kid and, and feeling that level of hostility and then hearing a teacher say that somebody with power to S to say that\n",
      "\n",
      "1 (12m 17s):\n",
      "I absolutely don't want to stop going. And I told my dad, I didn't want to go back. And dad gave me three options. He said, you can come back here and compete against other black kids who are going to need scholarships to go to school. He could go up and he could floor the teacher and a secret taliation or I could go back and I could understand. And this is something that if you say to many black people, they will finish this sentence for you. It's not what they call you. It's what you answer to. And that was my first big lesson from dad that I should not listen to what they called me.\n",
      "\n",
      "1 (12m 59s):\n",
      "I could only be what I answer to in life.\n",
      "\n",
      "3 (13m 2s):\n",
      "Hm. So you, you graduated high school and then you went off, you want to have to college to North Carolina, a and T what do you, what do you remember about that time?\n",
      "\n",
      "1 (13m 13s):\n",
      "I loved that campus and that's where my older siblings had gone. It, it was such a wonderful time to be at North Carolina, a and T when I graduated, everybody was full of promise in ideas about what we could do. So much of my adult framing happened on that campus. And by the time I started my own business, I was able to reach back on what I had learned in my home, which is where I gained. Most of my business education was in my home and the ability to, to meet new people and not as strangers that I learned at a and T, that was so wonderful.\n",
      "\n",
      "1 (13m 54s):\n",
      "Yeah.\n",
      "\n",
      "3 (13m 54s):\n",
      "And what did you do when he graduated?\n",
      "\n",
      "1 (13m 57s):\n",
      "First? I worked in the national Academy of sciences in Washington, DC. I worked there for about a here and a half, and I went home to visit mom and dad. And I remember one morning, my mom and dad, when we were kids early morning was always there. Time. I told her dad left to work really early in the mornings. And their date time was early morning and they dated in the kitchen in the little they window, where she has the table year to this day. And I got up to walk through the hall and I saw them kissing and hugging just, just like teenagers.\n",
      "\n",
      "1 (14m 39s):\n",
      "And I thought, Ooh, you know, I'm an adult by then. Right? That was the last time my mom saw my dad alive. And I many times then have thought, if you have to say goodbye, what a wonderful wait for her to know that the last time he saw her alive, he held her and loved her. So richly and She him to leave a heart attack. My dad had taken two young men out on the waters in North Carolina, shrimp, boat, fishing. They want to shrink boat. And a storm came up.\n",
      "\n",
      "1 (15m 20s):\n",
      "My dad was taken in the storm. Wow. And so I remember my mom took her be at, for a couple of weeks. And I had booked a ticket to come to California to visit my sister Sandy. And I remember I told my mom, I'll stay here with you mom and help you. My mom had been married since she was a teenager. She'd never had any other boyfriend. And I knew there would be some heavy adjustment for her. And she said, no, I'm going to have to learn to live on my own. I better do it now. And then she said, the last thing dad would want is for me to stop you living your dream, because we certainly, we certainly have lived hours.\n",
      "\n",
      "3 (16m 8s):\n",
      "Wow. So you, you leave North Carolina. I mean, what, what a, what a moment in your life, in your mom's life in your family's life? I mean, this is like late seventies. Yeah. And so you come to LA to visit your sister and it was, it was supposed to be just a quick visit to, to, to see her or a couple of days or weeks or something like that.\n",
      "\n",
      "1 (16m 33s):\n",
      "A couple of weeks planned. It was going to be a good visit. Two weeks back. There was a long time. Still is.\n",
      "\n",
      "3 (16m 38s):\n",
      "Yes. Yeah. And you were just a kid. I mean, you were in your mid twenties, right. Or early twenties at that point, I was hot and popping.\n",
      "\n",
      "1 (16m 48s):\n",
      "Although, let me tell you when I got to LA, I didn't think so. On the East coast, I thought I was all of that. And I got to LA and I saw all these women who worked without pantyhose on, they carry purses with somebody else's name on it. And I was called in at Louis Vuitton and they were all, all the black people, all the black women I saw were, were, were like fabulous, gorgeous women. And I felt like this, a little nappy headed colored girl coming out of North Carolina amongst all these fabulous people. No, I didn't feel all hot popping them.\n",
      "\n",
      "3 (17m 25s):\n",
      "What was your first impression of, of LA at that time? And this is like the late seventies. It was\n",
      "\n",
      "1 (17m 30s):\n",
      "Palm trees, gorgeous, gorgeous Palm trees. And you know, you go up and drive all the way up and look out over the city. And it was just so beautiful. So beautiful. LA was wide open. It truly was a fairy tale kind of existence for me. And I wanted in on it. You want it to stay? I I didn't at first I needed to sustain my stay financially. Cause my sister kept saying stay I'm the first family member she's got out here from home.\n",
      "\n",
      "3 (18m 5s):\n",
      "And, and her husband, Tommy, he, he worked at, at, at billboard magazine. Is that it?\n",
      "\n",
      "1 (18m 11s):\n",
      "He did. He had worked at Motown for years and helped move Motown out to LA. And then he went to billboard. You know, his career had been in the music industry. I remember seeing his name scrolled a year. He died at the Grammys and Tommy invented what they call the hot one hundreds chart. Oh, the billboard hot 100. Yeah. Yes. To this day I keep meeting people who just have such great memories of him. And certainly I do. He was an incredible, incredible first-generation Irishman.\n",
      "\n",
      "3 (18m 43s):\n",
      "So you, you go out to LA to see your sister and her husband, your brother-in-law Tommy says, Hey, you know, I may have some temp work for you at, at, at billboard. And, and that's what you did. You kind of went to his office and worked, kind of worked for him.\n",
      "\n",
      "1 (18m 59s):\n",
      "Yeah. She and Tommy, we're going to a Nimic conference in Italy. And when he came back, I had reorganized things, put things into a form as I saw they should be. And he thought I worked magic. I thought I did what I was supposed to do. And he said, you know, you don't, you, you, you really are good Janice you should not go back without proving. You can make it on your own. And he was the one who seeded the idea that I should hang my own shingle.\n",
      "\n",
      "3 (19m 29s):\n",
      "You were looking for work. And your brother-in-law essentially says, Hey, Janice why don't you just open up a business and you've got your job. You've got your own job.\n",
      "\n",
      "1 (19m 40s):\n",
      "He absolutely did. It was that simple for him. He later told me he saw so much enterprise entrepreneurship in me that I wasn't seeing in myself. So\n",
      "\n",
      "3 (19m 52s):\n",
      "Was the business idea that you guys started to talk about?\n",
      "\n",
      "1 (19m 56s):\n",
      "I actually started as a full-time agency, meaning what we were finding. I found people full-time jobs. You will like a head Hunter sort of, yeah. At the admin level,\n",
      "\n",
      "3 (20m 9s):\n",
      "Once you decided that you were going to start this company, this agency to help place people in jobs, what was your first move? I mean, w where did you get an office? How did you,\n",
      "\n",
      "1 (20m 20s):\n",
      "How did you start it? I made a deal with a guy who owned a rug shop and I set up office in front of the rug shop. It was a very, very beautiful in Beverly Hills, in Beverly Hills. Yeah. Location, location. Location is one of the Beverly Hills address to say, I had a friend who hooked me up with someone who had a Beverly Hills location. I was not looking at real estate that way, but it worked out perfectly. The stars were aligned for me.\n",
      "\n",
      "3 (20m 50s):\n",
      "And what did you call it? What'd you call the company ACT as in act one, like when you're in a movie,\n",
      "\n",
      "1 (20m 57s):\n",
      "You know, because it's located here in Los Angeles area. Many people thought that, but for me it was more around the biblical sense. The book of acts. Yeah.\n",
      "\n",
      "3 (21m 10s):\n",
      "So you get, get This office space and what you put in a desk, in a phone and there you go.\n",
      "\n",
      "1 (21m 16s):\n",
      "It was there. They didn't have to, it was there and it worked out very well for me. But I'll tell you something. When I did get my own office, that I remember the first day I got a fax machine. Do you know who Judy Jetson? In sure. Of course. Yeah. I thought I was Judy Jetson technology hit hard for me. Yes. That was incredible for me. And it also taught me the power of technology. And I think that's why over the years, my company has evolved more around how we build human friendly technology than anything else.\n",
      "\n",
      "3 (21m 53s):\n",
      "When you started act one, did you need a lot of, a lot of startup money to get it going?\n",
      "\n",
      "1 (21m 59s):\n",
      "I borrowed money from my mom and I had saved 900 and borrowed 600 from her.\n",
      "\n",
      "3 (22m 6s):\n",
      "But so you had $1,500 to start this business. Yep. And that was going to get you to the office, a lease on the office and I guess a phone, but here's what I'm, I'm trying to understand. You are so young and you just essentially just got there. So how did you even start? How did you even find people to, to recruit, to say, Hey, I, you know, you're looking for work. I can help you.\n",
      "\n",
      "1 (22m 31s):\n",
      "Well, finding people to place was not an issue during that time, finding the chops was, and the core of our business remains today as it was when I was one desk. And that's understanding the power of the interview. And just, I mean, you've talked with me for a few minutes now, you know, a lot about me and you know, a lot of resources to go, to gain more information. And that's the way the interview worked as well. So the power of that interview enabled me to make sure that I understood the individual who was looking for work. I knew where they'd already worked. I knew if they were already employed that once they left, there was going to be an opening there. And because of the rich network that I was able to achieve in the social side of how I'd lived with my brother and sister, I knew people who were looking for assistance, people who were looking for people to hire and built it on that in the early days, I call it the womb word of mouth, baby.\n",
      "\n",
      "1 (23m 30s):\n",
      "I got, I got most of my leads toward opportunities to fill positions. And most of the least toward applicants, by word of mouth,\n",
      "\n",
      "3 (23m 40s):\n",
      "So your business model was you help companies fill these jobs mostly jobs\n",
      "\n",
      "2 (23m 46s):\n",
      "And they would pay you a fee. And how were you able to guarantee that the person you were offering was going to work out?\n",
      "\n",
      "1 (23m 54s):\n",
      "Oh my goodness. There were so many different guarantees that were offered in our industry back then. And I haven't thought about this for years, but that's the risk to take on making sure that you're making a good placement. And that's why I say we have to focus on the applicant because you, you offered people money back. If they didn't work out, then, you know, you had, you had a reduction in that fee. You got, and you got, you had to pay that money back if they didn't last. So nobody want it to do that.\n",
      "\n",
      "2 (24m 23s):\n",
      "Were you, were you profitable pretty quickly or did it take some time\n",
      "\n",
      "1 (24m 30s):\n",
      "Actually thinking back on it hitting profitability in a year seems pretty good, but living through it, it didn't. But remember I was in a very low overhead business and I was in a business that had a high transaction. I was as good as my effort allowed me to be. Yeah.\n",
      "\n",
      "2 (24m 56s):\n",
      "In those early days, as you, you know, as you start to grow, things were, things were like kind of hectic. And then in the midst of this, you like, you got buried to write like in the early eighties.\n",
      "\n",
      "1 (25m 8s):\n",
      "Well, I met my husband at an industry conference, a Bernie and I saw this guy and he was really handsome to me. He had this presence about himself and he also spoke with this English accent, but he liked me a lot more than I did him. I noticed him. I can't say I didn't notice him, but he liked me a lot more than I liked him. And he, he chased me for a bit. And I found out that he was the founder of Apple one\n",
      "\n",
      "2 (25m 36s):\n",
      "And Apple. One was a temporary employment agency.\n",
      "\n",
      "1 (25m 40s):\n",
      "Apple one was Apple. One was a really respected, temporary and play a full-time placement agency doing business in California,\n",
      "\n",
      "2 (25m 50s):\n",
      "Married, essentially your competitor. How did you keep like work in personal life? Separate.\n",
      "\n",
      "1 (25m 55s):\n",
      "I think it would be the same as if we have been physicians or scientists. We're going to share a love of the industry and talk about it. And we're still competent, capable people building a business.\n",
      "\n",
      "2 (26m 10s):\n",
      "And so when was it clear to you that this wasn't just going to become sustainable, but this could actually become really pig business. Was it, was it within a year? Was it within two years? Was it after that?\n",
      "\n",
      "1 (26m 24s):\n",
      "No, no, no. I would say it was about, Oh, I want to say six or seven years in. And I remember a lady named Gwen Moore when Moore was any elected official Congresswoman out of California here. And she had been very in championing and I believe she was one of the original authors of legislation that require a public utility companies in California to have diversity spend, because she said your rate payers are diverse, so you should be doing business with them. And she contacted me and she said, I want to come see you.\n",
      "\n",
      "1 (27m 5s):\n",
      "And she'd met with me. And she told me that I had an obligation to get certified, to, to do what as a minority woman owned company. And I didn't want to do that. I was doing very well. Thank you. And she said, no, it's not about you doing better. Although you will. She said, it's about you creating the opportunity for others who won't get a shot. She said, we need some strong businesses that are run by women and run by minorities to certify and go in and do business as such to open the door for others. And even when she was arguing the point as eloquent, as clear as she is, I wasn't getting the point because initially I thought that certification was a strip search.\n",
      "\n",
      "1 (27m 57s):\n",
      "I needed to open up my business to total exposure to someone, to in return, have them come back and say, yes, you are a minority. Yes, you are a woman. And yes, you are running this business. And it just did not align with how I felt that I want it to be measured when they come back in just a moment,\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "{'score': 'yes'}\n",
      "\n",
      "0 (32m 36s):\n",
      "Just keep thinking. I mean, it seems like you were growing pretty fast. So\n",
      "\n",
      "2 (32m 40s):\n",
      "Your head must of been spinning.\n",
      "\n",
      "1 (32m 42s):\n",
      "Well, let me tell you two things about that one. The big thing for me was not so much the exponential growth as it was the adoption of doing the temp work and bringing tip into the fold temp work. You're paying people long before the client pay you. And so there's another element of risk in there. You are also the employer of that worker. And so you assume the employer, your risks. So you've got to dynamic new and very strong differences in place here. And you're competing against larger entities who are going in and able because of the sheer scale to offer a much better pricing.\n",
      "\n",
      "1 (33m 27s):\n",
      "And you are for, for me and many companies like me, woman owned you. We're the minority company where you were the diverse company. So you were getting one 10th of that business against one larger vendor. And I still had to meet the same pricing as a larger company to do that efficiently, to do that sustainably. I needed to integrate technology in a different way, and I didn't find any over the shelf technology. So that's when I started to hire people and we built our own technology solutions.\n",
      "\n",
      "2 (33m 59s):\n",
      "And I guess it was just mentioned here Janice that, that some of the solutions that you came up with internally, these actually became a huge part of your business.\n",
      "\n",
      "1 (34m 8s):\n",
      "Yeah, so our technology sweet is called a celebration and it really was about that detailed reporting. We were able to give to clients that they weren't getting anywhere else. And so I had a client up North who had a very bad experience with a publicly held company, call me on a Friday morning and ask if I would come up. So I actually ran over to the airport with my brother, another employee. And the three of us went up North, but in Northern California to the client. And we found out that they had an issue. If we are This publicly held company was not negotiating well with them. And he walked out and they had several hundred tips who had to be paid.\n",
      "\n",
      "1 (34m 50s):\n",
      "And we had to get all of that work together, transition those people. And on Monday morning, all of the work was done. Everyone was paid and they have reports on their desk. Wow. I got a call from the lady who was head of HR and said, my goodness, Janice how were you able to do this? And I explained that we used for her, what we use in our own company technologies that enable us to do that. And she said, we've been working with this company for 12 years, and they've never been able to give us a detail of reporting that you have here. And they started to want conversations around how to buy my technology. And my brother called and said, Janice, let's pause.\n",
      "\n",
      "1 (35m 32s):\n",
      "Don't sell them to technology, sell them the service. Yeah. I said, Carlton, what are you? He said, no, don't sell your technology to them, sell them the service. And we went up and had a different conversation with them and they became our first acceleration customer. Hmm.\n",
      "\n",
      "3 (35m 52s):\n",
      "And, and I guess I should mention here, Janice that you mentioned your brother, several members of your family actually work for the company for X. Y.\n",
      "\n",
      "1 (35m 59s):\n",
      "Absolutely. My sister Sandy was my first employee. And since then I went on to hire seven of the siblings in to my business. And they have just been incredible for all of those emotional and those value support systems that you need in place. As you're growing a business up, everyone needs that. I needed that. And I have that in siblings. Now, one of the things that I insist it from my family members before they could come in to my company is that they either had to work for three years in a larger company somewhere else, or have three promotions before they could come to work for me. I wanted to make sure that they brought into my business.\n",
      "\n",
      "1 (36m 40s):\n",
      "Some learning, not just the abilities to sustain for three years, but the ability to have learned and have grown within someone else's organization, and then bring that value into mine.\n",
      "\n",
      "3 (36m 54s):\n",
      "I guess, at a certain point, this is like 2007, 2008. You know, you're a business is worth well over half a billion dollars by that point. And you decide to merge with Bernard's business with your husband's business. Tell me why, why did that happen? Was it just a sort of a natural point to kind of bring your businesses together?\n",
      "\n",
      "1 (37m 17s):\n",
      "When Bernie and I were leaving conference in San Diego, we decided to take the scenic route back to LA and we were having a discussion about our children. And I remember my brother has said to me, Carlton has said, you know, Janice you and Bernie have something that nobody else in this industry has. And I asked him what he meant. He said, well, you guys work from completely different sets of strengths, often time. And if you were to combine that, that could be dynamic in the industry.\n",
      "\n",
      "1 (37m 59s):\n",
      "And I thought that was nice. And I mentioned it to Bernie. And Bernie said, that makes sense. I said, but it's not enough to talk to merge our companies. And we talked more. And then I said, you know, burning. I think the thing that would make it interesting for me is that our son not have to decide which one of us he wants to work for. Because by then our son was looking at our industry as a future for himself. And he had worked in my company. He at worked in his dads company as a kid. And I said, Bernie, I don't think he should have to make a choice.\n",
      "\n",
      "1 (38m 38s):\n",
      "And it is a lot better succession planning for us to go ahead and do this. Now let's not have him have to do it later. And that was the decision that was thought around how we would blend the company.\n",
      "\n",
      "2 (38m 56s):\n",
      "Jess, I'm wondering, you know, looking back as, as the company grew for a long time, you were a minority within a minority. You, you were a small company with big competitors. You're a woman of color or in an industry that was, I'm assuming dominated by, by a white man. Did you run into circumstances? A lot of circumstances where you, you were judged because of who you are or, or, or were things were made much harder because of who you are\n",
      "\n",
      "1 (39m 26s):\n",
      "Very often, very often I ran into that. I'll tell you candidly, and I'm not proud of it. There were times when I would gift my intelligence to other members of my team and have them go in and make a presentation or them make the pitch so that the client wouldn't have to In interact directly with me as an African American or as a female.\n",
      "\n",
      "2 (39m 50s):\n",
      "But, but because you thought they wouldn't want to,\n",
      "\n",
      "1 (39m 54s):\n",
      "In some instances, I thought it, in other instances, I knew it\n",
      "\n",
      "2 (39m 58s):\n",
      "Is that if they saw an African-American woman making the pitch, they, they wouldn't want to work with you.\n",
      "\n",
      "1 (40m 7s):\n",
      "I think the questioning and the scrutiny would be different. There would be more a question of, can we then, how will we, right. Whereas if I sent someone different on my team, in the question's around, how are we going to do this happened a lot quicker and the conversation, and there are women who will tell you Today, that's still the case.\n",
      "\n",
      "2 (40m 33s):\n",
      "How, how have you sort of grown as a leader over that time? I mean, were there things that you did earlier on that were mistakes that you learned from and you thought, well, you know, that's not the way to go.\n",
      "\n",
      "1 (40m 47s):\n",
      "Certainly I've made mistakes, the bigger of my mistakes, where are the mistakes I made, where I have held myself back because of things that were latent from my childhood and sometimes very active, you know, in businesses, around the isms, whether that be racism or sexism or whatever. But those have been where I think my bigger errors have been made.\n",
      "\n",
      "2 (41m 13s):\n",
      "When you, you say, when you held yourself back by, for example, by not taking those meetings or by sending somebody else and your, your place, what, what other, what other ways did you hold yourself back?\n",
      "\n",
      "1 (41m 26s):\n",
      "Holding myself back from taking risks around expanding or investments that weren't right in the core of what I'm doing. But I always, I always saw myself as, you know, driving in my lane and understanding where my lane was.\n",
      "\n",
      "3 (41m 46s):\n",
      "Yeah. I mean, I mean, it sounds like even though you have had incredible success, you by nature, you're not a sort of a kamikaze risk-taker,\n",
      "\n",
      "1 (41m 56s):\n",
      "I'm not going to be the bull in the China shop. If that's what you're saying, I'm not going to rush through and, you know, break things on the way towards a higher goal. Perhaps the one decision I would change if in my career would be that I would forgive myself for being smart and being female a lot sooner. Everything else I think has been integrated toward the good of a solid business that has a strong future\n",
      "\n",
      "3 (42m 28s):\n",
      "Today act. One is either the largest or certainly one of the largest privately held women and minority owned workforce management companies in the U S your, you and your family owned this company. I believe a a hundred percent outright. There's a billion dollar business. I mean, that's, that's amazing. I mean, it's me think about that for a moment. You came to LA for a two or three week trip and built a billion dollar business.\n",
      "\n",
      "1 (42m 59s):\n",
      "It's a blessing. This is what we Built. This is what all of the people who really believed in me and trusted in me have built. And in our company, we teach that there are five things you can't teach people, you can't teach people experience. You can't teach people common sense. You can't teach people confidence. You cannot teach people anything. If they don't want to learn it and you can't teach them anything, if they know it all. And I think when I look at where you see the amazement in building a billion dollar enterprise, I think is because those things that you can't teach I've learned.\n",
      "\n",
      "3 (43m 44s):\n",
      "Hmm. How much of all that you've achieved do you think is because of the hard work you put in and your determination and your skill and how much, just because of luck.\n",
      "\n",
      "1 (44m 0s):\n",
      "I don't think luck had anything to do with it. I do believe that I've been blessed and I have received those blessings by honoring them with hard work. All of the challenges, all of the people, all of the clients and applicants, my life has been a collitus topic opportunity.\n",
      "\n",
      "3 (44m 27s):\n",
      "I keep going back to that moment where you're your mom in morning, told\n",
      "\n",
      "2 (44m 32s):\n",
      "You to leave, told you to go. She did that. Your dad would want you to pursue whatever it was that you were going to pursue, and you didn't know what that was going to be. Right? I mean, yeah. Can you imagine what your dad would make of this? Like his daughter running the largest women and minority owned business of this kind in the world? I mean, what do you think he would, he would say or think\n",
      "\n",
      "1 (45m 2s):\n",
      "I absolutely can imagine what dad would say or what dad would think. My dad is the one who told us it was our attitude, not our aptitude. My dad was the one who told us education is freedom. My dad was the one who taught us that we wake up on purpose, you know? And, and, and, and I, I think about him so often. And you can't not think about him when you're with mom. I was a voted princess in high school, and I remember asking for a new gown to wear on a float. And dad said, Oh, we can't afford it. We've worked so hard for you guys. And I remember my mom putting her hand on my dad's hand and said, no, daddy, they called each of the mommy and daddy.\n",
      "\n",
      "1 (45m 47s):\n",
      "She said, no, daddy, don't tell Janice. We work hard for our kids. We work hard because of the decisions we made. And I kept that in my mind, as I built my business and I've made a point never to tell my children, I don't work hard to give them what they want. I work hard because of the decision I made, you know, and for me, it's a joy, it's a joy to do what I do.\n",
      "\n",
      "2 (46m 17s):\n",
      "That's Janice, Bryant Howroyd founder and CEO of F1 Group. And unfortunately, we've got to a sad update to the story. Earlier this year, Janice's husband Bernard passed away, deep in suffering from Alzheimer's for several years and an interview after his passing, Janice said, quote, I would never say Bernie was the most perfect creature, but he was the most perfect husband. And Pete loved me. Well, and thanks so much for listening to the show this week, you can subscribe wherever you get your podcasts. You can also write to us at the HIV team at NPR dot org. And if you want to follow us on Twitter, we're at How I Built This with Guy Raz and an Instagram is at Guy dot Ross.\n",
      "\n",
      "2 (47m 5s):\n",
      "Please also remember to visit donate.npr.org/ Built to give directly to a local NPR member station. This episode was produced by James DeLuise with music composed by rep teen Arab Louis. Thanks also to Ferris Safari, Liz Metzger, Derek gales, Julia Carney, JC Howard Neeva grant, and Jeff Rogers. I'm Guy Ross. And you've been listening to how I built this.\n",
      "\n",
      "4 (47m 35s):\n",
      "This is NPR\n",
      "\n",
      "5 (47m 40s):\n",
      "2020, had a lot of us rethinking our lives. 2021. Life kit wants to help you make those changes, whether they're big or small, all of this January life kit, we'll give you smart tips to think through your next decision. Listen now to the life kid podcast from NPR.\n",
      "\n",
      "6 (47m 58s):\n",
      "This message comes from NPR sponsor Adyen, the future-proof payments platform. Adyen connects you with customers around the world and makes it simple to accept all kinds of payments, an app online in store touch-free and beyond with a single solution. Keep customers happy in your business. Growing with Adyen, business, not boundaries, visit a D Y E n.com/ NPR to learn more.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "{'score': 'no'}\n",
      "\n",
      "0 (2m 52s):\n",
      "Ron NPR is how I built this a show about innovators, entrepreneurs, idealists, and the stories behind the movements. They Built\n",
      "\n",
      "2 (3m 4s):\n",
      "I'm Guy rosin on the show today, I would temporary gig as a secretary inspired Janice Bryant Howroyd to build an employment agency that turned into an empire and B Janice the first African-American women in history to own a billion dollar business. So hiring employees is one of the hardest things about running a business. If you've listened to this show long enough, you've heard founders talk about how when their company started to scale, hiring enough, good people consumed so much of their time because hiring requires patience and research, and yes, luck, even the best applicants sometimes flail after they start.\n",
      "\n",
      "2 (3m 49s):\n",
      "And this was an insight that Janice Bryant Howroyd came across early in her career after working as a secretary for her brother-in-law it was the late 1970s and her brother-in-law Tom Noonan needed an assistant for his growing role at billboard magazine and Hollywood. Janice noticed that lots of other executives in Hollywood needed clerical help too. And they needed it fast. They just didn't have a time to find the right people. So with all of her money, which was $1,500, Janice rented a small space in front of a rug shop in Beverly Hills. And she started an employment agency at the time. Janice was a young African-American woman starting out in an industry dominated by older white men.\n",
      "\n",
      "2 (4m 32s):\n",
      "Now, today her company act one group does an estimated billion dollars a year in sales act, one handles, hiring and recruiting for all kinds of industries. And it also provides a lot of backend services like employee screening and payroll, but when it began, it was just one office, one phone, and Janice she opened that first office in a city. She barely knew when she first arrived to LA in her mid twenties, Janice had spent most of her life in a very different place from Hollywood. She grew up in the 1950s and sixties in a big family in the small town of Tarboro. North Carolina\n",
      "\n",
      "1 (5m 14s):\n",
      "Grew up on the other side of the tracks. And that's literally the case because we did have a train that ran through our town. We were a town that was incorporated in the 17 hundreds. So a train track in the middle of the town did indicate some level of prosperity. Now we were segregated. Make no make no mistake about that. But we had that Southern etiquette that went along with it. So there was a politeness that occurred amongst people, but we also were very aware of the injustices. Hmm.\n",
      "\n",
      "2 (5m 48s):\n",
      "How many, how many brothers and sisters did you have growing up?\n",
      "\n",
      "1 (5m 51s):\n",
      "Five brothers, five sisters, all type a\n",
      "\n",
      "2 (5m 54s):\n",
      "So 11 kids in your house. Yeah. You say that, like, that sounds\n",
      "\n",
      "1 (5m 58s):\n",
      "So much to you.\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "# llm = llm.with_structured_output(schema={\"score\": \"str\"}, method=\"json_mode\")\n",
    "# llm = OpenAI(model=\"gpt-3.5-turbo-0125\", response_format={ \"type\": \"json_object\" })\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "llm = ChatAnthropic(model=\"claude-instant-1.2\")\n",
    "prompt = PromptTemplate(\n",
    "    template=doc_grading_prompt,\n",
    "    input_variables=[\"transcript\", \"question\"]\n",
    ")\n",
    "chain = prompt | llm | JsonOutputParser()\n",
    "question = \"How much of your success do you think is because of luck and, and how much because of Either a hardworking skill.\"\n",
    "results = MONGODB_COLLECTION.aggregate([\n",
    "        {\"$vectorSearch\": {\n",
    "            \"index\": ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
    "            \"path\": \"embedding\",\n",
    "            \"queryVector\": embeddings.embed_query(question),\n",
    "            \"numCandidates\": 100,\n",
    "            \"limit\": 3,\n",
    "            \"filter\": {\"company\": \"ActOne Group\"}\n",
    "        }}\n",
    "    ])\n",
    "for doc in results:\n",
    "    score = chain.invoke({\"question\":question, \"transcript\":doc[\"transcript\"]})\n",
    "    print(score)\n",
    "    print(doc[\"transcript\"])\n",
    "    print(\"-------------------\" *10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Dict, TypedDict\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        keys: A dictionary where each key is a string\n",
    "    \"\"\"\n",
    "    keys: Dict[str, any]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The state of the graph\n",
    "    \n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    company = state_dict[\"company\"]\n",
    "    documents = MONGODB_COLLECTION.aggregate([\n",
    "        {\"$vectorSearch\": {\n",
    "            \"index\": ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
    "            \"path\": \"embedding\",\n",
    "            \"queryVector\": embeddings.embed_query(question),\n",
    "            \"numCandidates\": 100,\n",
    "            \"limit\": 3,\n",
    "            \"filter\": {\"company\": f\"{company}\"}\n",
    "        }}\n",
    "    ])\n",
    "    return {\"keys\": {\"documents\": documents, \"question\": question}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class QuestionAnswer(BaseModel):\n",
    "    # question: str = Field(description=\"Question that the host asked the guest\")\n",
    "    answer: str = Field(description=\"The guest's answer to the host's question\")\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The state of the graph\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains generation\n",
    "    \n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    question = state_dict[\"question\"]\n",
    "\n",
    "    ### everything below this is a placeholder right now, need to come back to this after the grading process\n",
    "\n",
    "    prompt = \"\"\"\n",
    "    You are tasked with analyzing podcast transcripts to extract discussions that are specifically related to a question posed by the host. Here is the question of interest:\n",
    "\n",
    "    {question}\n",
    "\n",
    "    Your objective is to find and extract the question that the host asked and the guest's answer that directly responds to this question. \n",
    "\n",
    "    Guidelines for the task:\n",
    "    - Identify the guest's answer that pertains to the question above. The relevant answer is typically located in the same or subsequent paragraphs following the question.\n",
    "    - Correct common transcription misinterpretations as you extract the guest's response.\n",
    "    - Exclude the original question from your output. Provide only the guest's answer, ensuring it is word-for-word, with necessary corrections for transcription errors.\n",
    "    - If a relevant question and answer pair is not found within the provided transcript segment, simply state \"Question/Answer not found.\"\n",
    "\n",
    "    Please concentrate on extracting the answer from the provided transcript segment below, with attention to detail and accuracy:\n",
    "\n",
    "    {transcript}\n",
    "\n",
    "    Respond in the following JSON format.\n",
    "\n",
    "    {response_template}\n",
    "\n",
    "    \"\"\"\n",
    "    parser = PydanticOutputParser(pydantic_object=QuestionAnswer)\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "    template=prompt,\n",
    "    input_variables=[\"transcript\", \"question\"],\n",
    "    partial_variables={\"response_template\": parser.get_format_instructions()}\n",
    "    )\n",
    "    \n",
    "    # llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "    # llm = ChatOpenAI(model=\"gpt-4-0125-preview\")\n",
    "    # llm = ChatMistralAI(\n",
    "    #     mistral_api_key=os.getenv('MISTRAL_API_KEY'), temperature=0, model=\"mistral-medium\"\n",
    "    #     )\n",
    "    llm = ChatOpenAI(api_key=os.getenv(\"PPLX_API_KEY\"), base_url=\"https://api.perplexity.ai\", model='mixtral-8x7b-instruct')\n",
    "\n",
    "    # Post-processing\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # Chain\n",
    "    rag_chain = prompt | llm | parser\n",
    "\n",
    "    # Run\n",
    "    generation = rag_chain.invoke({\"transcript\": documents, \"question\": question})\n",
    "\n",
    "    if generation == \"Question/Answer not found.\":\n",
    "        llm = ChatAnthropic(model=\"claude-instant-1.2\")\n",
    "        rag_chain = prompt | llm | StrOutputParser()\n",
    "        generation = rag_chain.invoke({\"transcript\": documents, \"question\": question})\n",
    "    \n",
    "    return {\n",
    "        \"keys\": {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args: \n",
    "        state (dict): The state of the graph\n",
    "    \n",
    "        Returns:\n",
    "        state (dict): Updates documents key with relevant documents\n",
    "    \"\"\"\n",
    "    print(\"---CHECK RELEVANCE---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "\n",
    "    doc_grading_prompt = \"\"\" You are evaluating a podcast transcript segment to determine if it contains a discussion relevant to a specific question. Your task is to assess whether the segment includes both a question from the host and an answer from a guest that relate closely to the provided relevant question.\n",
    "    Transcript segment:\n",
    "    {transcript}\n",
    "    Relevant question to compare:\n",
    "    {question}\n",
    "    Your job is to decide if the segment discusses a question and answer similar to the relevant question provided. You need to give a binary score indicating the relevance.\n",
    "    - If the segment contains a question and answer that closely match the relevant question, grade it as 'yes'.\n",
    "    - If the segment does not contain a question and answer that are relevant, grade it as 'no'.\n",
    "    Provide your assessment in the following JSON format, with only the 'score' key and your binary decision as the value. No preamble or explanation needed.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "    template=doc_grading_prompt,\n",
    "    input_variables=[\"transcript\", \"question\"]\n",
    "    )\n",
    "    llm = ChatAnthropic(model=\"claude-instant-1.2\")\n",
    "    chain = prompt | llm | JsonOutputParser()\n",
    "\n",
    "    relevant_documents = []\n",
    "    reserach = \"No\"\n",
    "    for doc in documents:\n",
    "        score = chain.invoke({\"question\":question, \"transcript\":doc[\"transcript\"]})\n",
    "        if score[\"score\"] == \"yes\":\n",
    "            relevant_documents.append(doc)\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            continue\n",
    "    \n",
    "    if len(relevant_documents) == 0:\n",
    "        reserach = \"Yes\"\n",
    "        print(\"---GRADE: NO DOCUMENTS RELEVANT---\")\n",
    "    \n",
    "    return {\"keys\": {\"documents\": relevant_documents, \n",
    "                     \"question\": question,\n",
    "                    \"research\": reserach\n",
    "                    }\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_query(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates question key with a re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "\n",
    "    # Create a prompt template with format instructions and the query\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are generating questions that is well optimized for retrieval. \\n\n",
    "        Look at the input and try to reason about the underlying sematic intent / meaning. \\n\n",
    "        Here is the initial question:\n",
    "        \\n ------- \\n\n",
    "        {question}\n",
    "        \\n ------- \\n\n",
    "        Formulate an improved question: \"\"\",\n",
    "        input_variables=[\"question\"],\n",
    "    )\n",
    "\n",
    "    # Grader\n",
    "    # LLM\n",
    "    llm = ChatMistralAI(\n",
    "        mistral_api_key=os.getenv('MISTRAL_API_KEY'), temperature=0, model=\"mistral-medium\"\n",
    "        )\n",
    "\n",
    "    # Prompt\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    better_question = chain.invoke({\"question\": question})\n",
    "\n",
    "    return {\n",
    "        \"keys\": {\"documents\": documents, \"question\": better_question}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer or re-generate a question for web search.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current state of the agent, including all keys.\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---DECIDE TO GENERATE---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    filtered_documents = state_dict[\"documents\"]\n",
    "    research = state_dict[\"research\"]\n",
    "\n",
    "    if research == \"Yes\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\"---DECISION: TRANSFORM QUERY and RESEARCH---\")\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Graph\n",
    "\n",
    "This just follows the flow we outlined in the figure above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generatae\n",
    "workflow.add_node(\"transform_query\", transform_query)  # transform_query\n",
    "# workflow.add_node(\"web_search\", web_search)  # web search\n",
    "\n",
    "# Build graph\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run\n",
    "# inputs = {\n",
    "#     \"keys\": {\n",
    "#         \"company\": \"ActOne Group\",\n",
    "#         \"question\": \"In reflecting on your path to success, how do you view the contributions of diligence, intellect, and fortuitous events? Are there particular moments or decisions that highlight the importance of these elements?\"\n",
    "#     }\n",
    "# }\n",
    "# for output in app.stream(inputs):\n",
    "#     for key, value in output.items():\n",
    "#         # Node\n",
    "#         pprint.pprint(f\"Node '{key}':\")\n",
    "#         # Optional: print full state at each node\n",
    "#         # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "#     pprint.pprint(\"\\n---\\n\")\n",
    "\n",
    "# # Final generation\n",
    "# pprint.pprint(value[\"keys\"][\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "---CHECK RELEVANCE---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: NO DOCUMENTS RELEVANT---\n",
      "---DECIDE TO GENERATE---\n",
      "---DECISION: TRANSFORM QUERY and RESEARCH---\n",
      "---TRANSFORM QUERY---\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ChatMessage' object has no attribute 'model_dump'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeys\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompany\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAirbnb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn reflecting on your path to success, how do you view the contributions of diligence, intellect, and fortuitous events? Are there particular moments or decisions that highlight the importance of these elements?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m     }\n\u001b[1;32m      6\u001b[0m }\n\u001b[0;32m----> 8\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/interviewEnv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:579\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, output_keys, input_keys, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28minput\u001b[39m: Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    577\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]:\n\u001b[1;32m    578\u001b[0m     latest: Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 579\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    581\u001b[0m         config,\n\u001b[1;32m    582\u001b[0m         output_keys\u001b[38;5;241m=\u001b[39moutput_keys \u001b[38;5;28;01mif\u001b[39;00m output_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput,\n\u001b[1;32m    583\u001b[0m         input_keys\u001b[38;5;241m=\u001b[39minput_keys,\n\u001b[1;32m    584\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    585\u001b[0m     ):\n\u001b[1;32m    586\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m latest\n",
      "File \u001b[0;32m~/anaconda3/envs/interviewEnv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:615\u001b[0m, in \u001b[0;36mPregel.transform\u001b[0;34m(self, input, config, output_keys, input_keys, **kwargs)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    614\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]]:\n\u001b[0;32m--> 615\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_stream_with_config(\n\u001b[1;32m    616\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    617\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform,\n\u001b[1;32m    618\u001b[0m         config,\n\u001b[1;32m    619\u001b[0m         output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m    620\u001b[0m         input_keys\u001b[38;5;241m=\u001b[39minput_keys,\n\u001b[1;32m    621\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    622\u001b[0m     ):\n\u001b[1;32m    623\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "File \u001b[0;32m~/anaconda3/envs/interviewEnv/lib/python3.10/site-packages/langchain_core/runnables/base.py:1494\u001b[0m, in \u001b[0;36mRunnable._transform_stream_with_config\u001b[0;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1493\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1494\u001b[0m         chunk: Output \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1495\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[1;32m   1496\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output_supported:\n",
      "File \u001b[0;32m~/anaconda3/envs/interviewEnv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:355\u001b[0m, in \u001b[0;36mPregel._transform\u001b[0;34m(self, input, run_manager, config, input_keys, output_keys, interrupt)\u001b[0m\n\u001b[1;32m    348\u001b[0m done, inflight \u001b[38;5;241m=\u001b[39m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mwait(\n\u001b[1;32m    349\u001b[0m     futures,\n\u001b[1;32m    350\u001b[0m     return_when\u001b[38;5;241m=\u001b[39mconcurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mFIRST_EXCEPTION,\n\u001b[1;32m    351\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m    352\u001b[0m )\n\u001b[1;32m    354\u001b[0m \u001b[38;5;66;03m# interrupt on failure or timeout\u001b[39;00m\n\u001b[0;32m--> 355\u001b[0m \u001b[43m_interrupt_or_proceed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minflight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;66;03m# apply writes to channels\u001b[39;00m\n\u001b[1;32m    358\u001b[0m _apply_writes(\n\u001b[1;32m    359\u001b[0m     checkpoint, channels, pending_writes, config, step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    360\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/interviewEnv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:698\u001b[0m, in \u001b[0;36m_interrupt_or_proceed\u001b[0;34m(done, inflight, step)\u001b[0m\n\u001b[1;32m    696\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m    697\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[0;32m--> 698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    699\u001b[0m         \u001b[38;5;66;03m# TODO this is where retry of an entire step would happen\u001b[39;00m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/interviewEnv/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/anaconda3/envs/interviewEnv/lib/python3.10/site-packages/langchain_core/runnables/base.py:4060\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4054\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4055\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4056\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4057\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4058\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4059\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4060\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4061\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4062\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4063\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4064\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/interviewEnv/lib/python3.10/site-packages/langchain_core/runnables/base.py:2056\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2054\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2055\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2056\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2058\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2059\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2060\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2061\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2062\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2063\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2064\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/interviewEnv/lib/python3.10/site-packages/langchain_core/runnables/base.py:3504\u001b[0m, in \u001b[0;36mRunnableLambda.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3502\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke this runnable synchronously.\"\"\"\u001b[39;00m\n\u001b[1;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3505\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3506\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3507\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3508\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3509\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3511\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   3512\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke a coroutine function synchronously.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3513\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `ainvoke` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3514\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/interviewEnv/lib/python3.10/site-packages/langchain_core/runnables/base.py:1243\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1239\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   1240\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(var_child_runnable_config\u001b[38;5;241m.\u001b[39mset, child_config)\n\u001b[1;32m   1241\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1242\u001b[0m         Output,\n\u001b[0;32m-> 1243\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1246\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1247\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1251\u001b[0m     )\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1253\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/interviewEnv/lib/python3.10/site-packages/langchain_core/runnables/config.py:326\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    325\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 326\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/interviewEnv/lib/python3.10/site-packages/langchain_core/runnables/base.py:3378\u001b[0m, in \u001b[0;36mRunnableLambda._invoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   3376\u001b[0m                 output \u001b[38;5;241m=\u001b[39m chunk\n\u001b[1;32m   3377\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3378\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   3380\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3381\u001b[0m \u001b[38;5;66;03m# If the output is a runnable, invoke it\u001b[39;00m\n\u001b[1;32m   3382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "File \u001b[0;32m~/anaconda3/envs/interviewEnv/lib/python3.10/site-packages/langchain_core/runnables/config.py:326\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    325\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 326\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 37\u001b[0m, in \u001b[0;36mtransform_query\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Prompt\u001b[39;00m\n\u001b[1;32m     36\u001b[0m chain \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;241m|\u001b[39m llm \u001b[38;5;241m|\u001b[39m StrOutputParser()\n\u001b[0;32m---> 37\u001b[0m better_question \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeys\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m\"\u001b[39m: documents, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: better_question}\n\u001b[1;32m     41\u001b[0m }\n",
      "File \u001b[0;32m~/anaconda3/envs/interviewEnv/lib/python3.10/site-packages/langchain_core/runnables/base.py:2056\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2054\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2055\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2056\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2058\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2059\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2060\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2061\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2062\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2063\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2064\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/interviewEnv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:166\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    162\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    163\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    165\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 166\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    175\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/anaconda3/envs/interviewEnv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:544\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    538\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    542\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    543\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/interviewEnv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:408\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    407\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 408\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    409\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    410\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    412\u001b[0m ]\n\u001b[1;32m    413\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/anaconda3/envs/interviewEnv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:398\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 398\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m         )\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/anaconda3/envs/interviewEnv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:577\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m     )\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/interviewEnv/lib/python3.10/site-packages/langchain_mistralai/chat_models.py:297\u001b[0m, in \u001b[0;36mChatMistralAI._generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    296\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m--> 297\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m~/anaconda3/envs/interviewEnv/lib/python3.10/site-packages/langchain_mistralai/chat_models.py:241\u001b[0m, in \u001b[0;36mChatMistralAI.completion_with_retry\u001b[0;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mchat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_completion_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/interviewEnv/lib/python3.10/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/interviewEnv/lib/python3.10/site-packages/tenacity/__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/interviewEnv/lib/python3.10/site-packages/tenacity/__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    312\u001b[0m is_explicit_retry \u001b[38;5;241m=\u001b[39m fut\u001b[38;5;241m.\u001b[39mfailed \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fut\u001b[38;5;241m.\u001b[39mexception(), TryAgain)\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry(retry_state)):\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter(retry_state)\n",
      "File \u001b[0;32m~/anaconda3/envs/interviewEnv/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/anaconda3/envs/interviewEnv/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/interviewEnv/lib/python3.10/site-packages/tenacity/__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    384\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/interviewEnv/lib/python3.10/site-packages/langchain_mistralai/chat_models.py:239\u001b[0m, in \u001b[0;36mChatMistralAI.completion_with_retry.<locals>._completion_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mchat_stream(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/interviewEnv/lib/python3.10/site-packages/mistralai/client.py:147\u001b[0m, in \u001b[0;36mMistralClient.chat\u001b[0;34m(self, messages, model, tools, temperature, max_tokens, top_p, random_seed, safe_mode, safe_prompt, tool_choice, response_format)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchat\u001b[39m(\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    117\u001b[0m     messages: List[Any],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m     response_format: Optional[Union[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m], ResponseFormat]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    128\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletionResponse:\n\u001b[1;32m    129\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"A chat endpoint that returns a single response.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m        ChatCompletionResponse: a response object containing the generated text.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m     request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_chat_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafe_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msafe_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m     single_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, request, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv1/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m single_response:\n",
      "File \u001b[0;32m~/anaconda3/envs/interviewEnv/lib/python3.10/site-packages/mistralai/client_base.py:95\u001b[0m, in \u001b[0;36mClientBase._make_chat_request\u001b[0;34m(self, messages, model, tools, temperature, max_tokens, top_p, random_seed, stream, safe_prompt, tool_choice, response_format)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_chat_request\u001b[39m(\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     82\u001b[0m     messages: List[Any],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m     response_format: Optional[Union[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m], ResponseFormat]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     93\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m     94\u001b[0m     request_data: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m---> 95\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msafe_prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: safe_prompt,\n\u001b[1;32m     97\u001b[0m     }\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m         request_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\n",
      "File \u001b[0;32m~/anaconda3/envs/interviewEnv/lib/python3.10/site-packages/mistralai/client_base.py:74\u001b[0m, in \u001b[0;36mClientBase._parse_messages\u001b[0;34m(self, messages)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m messages:\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, ChatMessage):\n\u001b[0;32m---> 74\u001b[0m         parsed_messages\u001b[38;5;241m.\u001b[39mappend(\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_dump\u001b[49m(exclude_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         parsed_messages\u001b[38;5;241m.\u001b[39mappend(message)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ChatMessage' object has no attribute 'model_dump'"
     ]
    }
   ],
   "source": [
    "inputs = {\n",
    "    \"keys\": {\n",
    "        \"company\": \"Airbnb\",\n",
    "        \"question\": \"In reflecting on your path to success, how do you view the contributions of diligence, intellect, and fortuitous events? Are there particular moments or decisions that highlight the importance of these elements?\"\n",
    "    }\n",
    "}\n",
    "\n",
    "answer = app.invoke(inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuestionAnswer(answer=\"I don't think luck had anything to do with it. I do believe that I've been blessed and I have received those blessings by honoring them with hard work. All of the challenges, all of the people, all of the clients and applicants, my life has been a collitus topic opportunity.\")"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer['keys']['generation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---question--- When you think about the success of what you created, how much do you think it has to do with, with how hard you worked and your intelligence and how much do you think it had to do with luck?\n"
     ]
    }
   ],
   "source": [
    "client = pymongo.MongoClient(os.getenv('mdb_uri'))\n",
    "DB_NAME = \"hibt_prod_db\"\n",
    "COLLECTION_NAME = \"hibt_prod_collection\"\n",
    "ATLAS_VECTOR_SEARCH_INDEX_NAME = \"vector_index\"\n",
    "MONGODB_COLLECTION = client[DB_NAME][COLLECTION_NAME]\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "print(f'---question--- {question}')\n",
    "documents = MONGODB_COLLECTION.aggregate([\n",
    "    {\"$vectorSearch\": {\n",
    "        \"index\": ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
    "        \"path\": \"embedding\",\n",
    "        \"queryVector\": embeddings.embed_query(question),\n",
    "        \"numCandidates\": 100,\n",
    "        \"limit\": 4,\n",
    "        \"filter\": {\"company\": \"Wikipedia\"}\n",
    "    }}\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = list(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n2 (35m 30s):\\nyears. When you think about this thing that you built and your role in the history of the internet how much of the success of Wikipedia do you think it's because of your Brilliance and your hard work? And how much do you think is simply X of luck\\n\\n1 (35m 48s):\\na huge amount due to luck Brilliance and hard work? Okay, maybe not so much. Do you think a component of the success of Wikipedia is that I'm a very friendly and nice person and I'm very laid-back. And so therefore I was able to work in a community environment where people basically yell at you and you just have to kind of roll with it and you're in some sense a leader, but you can't tell anyone what to do their volunteers. So you have to work with love and reason to kind of move people along in a useful way. So I do think that I'm not irrelevant to the process. But I also think that you know, the community is amazing and the luck of the timing of really hitting that moment when it was possible to build\\n\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]['transcript']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interviewEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
